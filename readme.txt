Lesson01: 
	Topic:	Building a simple single layer perceptron with a fully connected hidden layer.
	Dataset: MNIST	

Lesson02:
	Topic:	Using a simple Convolutional Neural Network to enhance the accuracy
		and decrease the loss of the MNIST Dataset from Lesson01.
	Dataset: MNIST

Lesson03:
	Topic: Visualize filters of VGG16
	Dataset: VGG16

Lesson04:
	Topic:	Regularization and Dropout. Show overfit and how to avoid it.
	Dataset: MNIST, TBD (Sequential Data?)

Lesson05:
	Topic:	Transfer Learning. Recreate the MNIST CNN from Lesson02 with two parts
		of layers. Train the whole CNN on the Numbers 0-4. Freeze the Conv
		Layers and retrain the last fully connected layers with the Numbers
		5-9. Show the decrease in computation time while maintaining the accuracy
	Dataset: MNIST

Lesson06:
	Topic:	Tranfer Learning 2. Use a Large Network and retrain it to a Cat-Dog Net.
		Use FastAi Lesson 01 as an orientation point. Bring in Data Augmentation.
	Dataset: Cat-Dog_Redux

LessonXX:
	Topic: Object Detection

LessonXX:
	Topic: Image Segmentation

LessonXX:
	Topic: Neural Translation

LessonXX:
	Topic: Generative Reinforcement Learning

LessonXX:
	Topic: Time series Learning

LessonXX:
	Topic: Self driving RC Car

LessonXX:
	Topic: Text Mining Twitter (Disaster Example) 

LessonXX:
	Topic: Neural style transfer.

LessonXX:
	Topic: Deep Dreams in Keras

LessonXX:
	Topic: Neural Doodle

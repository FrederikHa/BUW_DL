Lesson01: 
	Topic:	Building a simple single layer perceptron with a fully connected hidden layer.
	Dataset: MNIST	

Lesson02:
	Topic:	Using a simple Convolutional Neural Network to enhance the accuracy
		and decrease the loss of the MNIST Dataset from Lesson01.
	Dataset: MNIST

Lesson03:
	Topic: Building a multi layer CNN from scratch. Train on cat-dog dataset and vizualise Filters
	Dataset: Cat-Dog_Redux

Lesson04:
	Topic:	Regularization and Dropout. Show overfit and how to avoid it.
	Dataset: Cat-Dog_Redux, TBD (Sequential Data?)

Lesson05:
	Topic:	Transfer Learning. Recreate the MNIST CNN from Lesson02 with two parts
		of layers. Train the whole CNN on the Numbers 0-4. Freeze the Conv
		Layers and retrain the last fully connected layers with the Numbers
		5-9. Show the decrease in computation time while maintaining the accuracy
	Dataset: MNIST

Lesson06:
	Topic:	Tranfer Learning 2. Use a Large Network (vgg?) and retrain it to a Cat-Dog Net.
		Use FastAi Lesson 01 as an orientation point. Bring in Data Augmentation.
	Dataset: Cat-Dog_Redux
	
Lesson07:
	Topic:	CAM (Class Activation Mapping). Show regions of interest in the Cat-Dog_Redux CNN from Lesson06.
		Does it predict with the right areas? Show a bad example with adversarial images.
	Dataset: Cat-Dog_Redux, MNIST(?)

LessonXX:
	Topic: Object Detection

LessonXX:
	Topic: Image Segmentation
	
LessonXX:
	Topic: Neural style transfer.

LessonXX:
	Topic: Deep Dreams in Keras
	
LessonXX:
	Topic: Self driving RC Car

LessonXX:
	Topic: Neural Translation

LessonXX:
	Topic: Time series Learning

LessonXX:
	Topic: Text Mining Twitter (Disaster Example) 

LessonXX:
	Topic: Neural Doodle
